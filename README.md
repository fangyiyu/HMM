# Copyright:

The notebooks in this repo are all completed assignments from deeplearning.ai NLP specialization courses, including the following:
1. [Building a n-gram language model for auto-completion.](https://github.com/fangyiyu/NLP_sourcecode/blob/master/Autocomplete%20using%20N-gram%20language%20model.ipynb)
2. [HMM and Viterbi algorithem.](https://github.com/fangyiyu/NLP_sourcecode/blob/master/HMM%20and%20Viterbi%20algorithem.ipynb)
3. [Continuous bag-of-words.](https://github.com/fangyiyu/NLP_sourcecode/blob/master/Implementing%20CBOW%20from%20scratch.ipynb)
4. [Implementing Logistic Regression](https://github.com/fangyiyu/NLP_sourcecode/blob/master/Logistic%20Regression%20on%20Sentimental%20Analysis.ipynb)[and Naive Bayes on sentimental analysis.](https://github.com/fangyiyu/NLP_sourcecode/blob/master/Naive%20Bayes%20on%20Sentimental%20Analysis.ipynb)
5. [Naive Machine translation using KNN.](https://github.com/fangyiyu/NLP_sourcecode/blob/master/Machine%20translation%20using%20word%20embeddings%20and%20KNN.ipynb)
6. [Principle Components Analysis.](https://github.com/fangyiyu/NLP_sourcecode/blob/master/Word%20vectors%20and%20PCA.ipynb)
7. [Auto-correction using Bayes rules.](https://github.com/fangyiyu/NLP_sourcecode/blob/master/autocorrection.ipynb)
8. [Sentimental Anaylysis using DNN.](https://github.com/fangyiyu/NLP_sourcecode/blob/master/Sentimental%20analysis%20with%20DNN%20using%20Trax.ipynb)
9. [Machine Translation using GRU.](https://github.com/fangyiyu/NLP_sourcecode/blob/master/Machine%20translation%20using%20GRU.ipynb)  
10. [Distinguishing question duplicates using Siamese model.](https://github.com/fangyiyu/NLP_sourcecode/blob/master/Distinguishing%20question%20duplicates%20using%20Siamese%20model.ipynb)  
11. [Name Entity Recognition using LSTM.](https://github.com/fangyiyu/NLP_sourcecode/blob/master/Name%20Entity%20Recognition%20using%20LSTM.ipynb)  
12. [Bleu score from scratch.](https://github.com/fangyiyu/NLP_sourcecode/blob/master/Bleu%20score%20from%20scratch.ipynb)  
13. [Neural Machine Translation with Attention.](https://github.com/fangyiyu/NLP_sourcecode/blob/master/Neural%20Machine%20Translation%20with%20Attention.ipynb)  
14. [Transformer from scratch.](https://github.com/fangyiyu/NLP_sourcecode/blob/master/Transformer%20from%20scratch.ipynb)  
15. [Bert from scratch.](https://github.com/fangyiyu/NLP_sourcecode/blob/master/BERT%20from%20scratch.ipynb)



# References:
1. [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (Raffel et al, 2019)](https://arxiv.org/abs/1910.10683)  
2. [Reformer: The Efficient Transformer (Kitaev et al, 2020)](https://arxiv.org/abs/2001.04451)  
3. [Attention Is All You Need (Vaswani et al, 2017)](https://arxiv.org/abs/1706.03762)  
4. [Deep contextualized word representations (Peters et al, 2018)](https://arxiv.org/pdf/1802.05365.pdf)  
5. [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Devlin et al, 2018)](https://arxiv.org/abs/1810.04805)  


